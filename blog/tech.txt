-- 
Can python make use of multiple cores? python multiprocessing?

Q: in your java project, is memory an issue?

You can read up Filecoin and IPFS

-- new: multiprocessing.pm
This module supports spawning processes using an API similar to the threading module. The multiprocessing package effectively side-stepping the Global Interpreter Lock by using subprocesses instead of threads. Due to this, the multiprocessing module allows the programmer to leverage multiple processors on a given machine. 

-- NEW: get short git hash from any ref
shortRef=$(git rev-parse --short $origRef)

-- new: c++ exceptions!=as useful as in newer languages
C++ has a tradition of UDB and USB. low level errors are often left to the hardware or the OS. See https://btv-gz.dreamhosters.com/wp-admin/post.php?post=954&action=edit
exceptions create pitfalls in dtor, swap etc
exception passing is trick and a major source of bug
exceptions affect efficiency and performance (the j4 C/C++) and is often disabled in gcc, or avoided. Ironically, when it comes to exceptions, arguably the most useful language feature is 'noexcept'.
Standard library uses very few exceptions.

.. C integration .. Many real world projects have substantial codebase in C, and often adopt C programming traditions.
C doesn't support exceptions. 
Exceptions are a high-level langage feature. 
Low-level langages like C use error code.

-- new: GPGPU: learning notes
A GPU is a programmable processor on which thousands of specialized cores (thread count is 10x higher) run simultaneously in massive parallelism. Although GPUs usually operate at much lower clock speeds than CPUs, a GPU has a massive amount of cores that help with multitasking.

Designed to work with programming languages such as C/C++, CUDA is an accessible platform, requiring no advanced skills.

CUDA accelerates many applications, including numerical analytics, scentific computing, and deep learning, encryption, . 

A "stream" is a set of records that require similar computation. Streams provide data parallelism. A "Kernel" is a function applied to each element in the stream. In the GPUs, vertices and fragments are the elements in streams and vertex and fragment shaders are the kernels to be run on them. 

Ideal GPGPU applications have large data sets, high parallelism, and minimal dependency between data elements.

GPU computing refers to Graphics Processing tasks, using graphics jargons; GPGPU refers to non-graphic computing.

Energy efficiency -- GPUs perform much more work for every unit of energy than CPUs. That makes them key to supercomputers that would otherwise push past the limits of todayâ€™s electrical grids.

x% of the blockchain mining computations can use gpgpu, but the other (100-x)% can only run on CPU.

-- new:
digital currenc, virtual currency can be unrelated to blockchain. The frequent flier miles, or ccard reward points can be transferred or traded. They can become virtual currencies.

I prefer the words "blockchain currency" and "crypto-currency"
-- new: 
seg fault is an immediate response or async, based on signals?
Is core-dump performed by a signal handler?

I think my linux book has some details.

-- new: unix domain sockets vs sockets to localhost

-- new: 
one process can send a signal to another process. This is a simple yet well-supported, proven IPC mechanism.

-- fuxi: uniqPtr MktVal imt sharedPtr
.. due to move-semantics, which is a hot favorite IV QQ topic

--fuxi
signals in windows? not popular

real time signals are a new Posix concept... not really low-latency.
Low latency sockets don't rely on signals for immediate action

Most signals are self-sent from the same process. I think timer alarm is one. Sockets is another example. However, keyboard signals like Ctrl-C are not self-sent.

--fuxi: sys call is not always hardware related. my linux book has a section on sys calls related to signals!

--new: malloc != syscall
is malloc a kernel service, syscall or a userland function offered in a regular library?

Can I implement my own DAM without involving the kernel? I would say some kernel sys call is needed.

Is there a cpu instruction for heap allocation? Does cpu care about heap vs stack? Yes the CPU cares about the stack. There are dedicated stack-registers in the CPU
whilst the stack space is managed via the CPU, the heap is not managed automatically. The size of the heap can also be considerably larger than the stack and the allocation of memory must be performed manually within a program. Memory also has to be freed manually, again unlike the stack.

Can check my linux book or google on brk()

-- fuxi: set in python
set is the least used builtin container. I seldom use it, becasue dictionary is always richer.
( Similarly, list is always richer than tuple, except when used as dict keys. )

One advantage of set over dict or list -- set-comprehension to create a hashset
