-- new: uniqPtr MV imt sharedPtr
.. due to move-semantics, which is a hot favorite IV QQ topic
--new: raii for array-new
std::vector ! see [[safeC++]] chapter on safer array
-- update
source .bashrc # executes each command in the rc, all within the current shell.
exec bash # starts a new shell, with an empty command history 
-- AI predicting security price
Decades ago, XML was touted as a miracle technology that can treat cancer.

Decades ago, options, swaps and other derivatives were touted as risk control products. 

How can AI predict stock price when the price depends on a few traders' decisions?

It's safe to define "price" as the LastTradePrice reported by recognized exchanges (excluding private, unregulated venues). we can pick any major stock symbol (or ETF , or a major FX pair)

In a simplest yet realistic scenario, there's a single $50M block trade executing on a $2B mid-cap stock, over a period of time. This buy order would drive up the price, but Cody the trader is careful to reduce market impact, by suspending execution once a while. Meanwhile there are other events such as market index movements, company news, sector news, economic events,,, to muddy the water and conceal the effect of the main driver.

An observing AI robot is unaware of Cody, but Cody knows there are AI robots and sharks out there. So Cody would try to confuse them. Cody also sends visible sell orders (executed or cancelled).

In a more realistic scenario, a stock is removed from Russell 2000. Many index-tracking ETFs would sell the stock. Let's assume our AI robot is unaware of this minor news item, which is likely the #1 driver for this stock price.

How meaningful, how scentific, how smart/stupid would be the prediction by our AI ? The mathematics have almost nothing to do with the reality. If you throw in 22 factors such as weather, GDP, unemployment, birth rate, Wechat usage stats, earthquake likelihood, twitter sentiment... a deep-learning robot may pick up some random factor as a driver, when the real driver is absent in the dataset.

-- update https://btv-gz.dreamhosters.com/wp-admin/post.php?post=25175&action=edit
BigData still favors java not python. (I wonder why.)

NoSql is bigData, not DS/ML. 

BigDataAnalytics includes BigData stack. 

Data Science probably favors python or R. ML is even less suitable for java.
ML includes classification, recommendation, ,, NeuralNetwork is part of ML

-- new data^information in bigData
Data needs processing before it becomes information.

I feel for high volume, high reliability, integrated, low-level "data" handling, java (and C++) are more suitable. See also https://btv-gz.dreamhosters.com/wp-admin/post.php?post=25175&action=edit

For high-level "information" analysis, python and R are more suitable. 

However, in reality my /feel/ might be wrong. Python might have big frameworks comparable to java’s.

ML Models and techniques are applied on information not raw data.

-- java 5 and java 8 are the two releases most quizzed in interviews.
How about java6/7/9? They introduce rather few visible changes. 

In fact, beside generics, java5's other changes are hardly visible. There are many additions in concurrency library but invisible/irrelevant to most developers.

-- new: java simplifying concurrency
concurrency is tricky. Java tries to simplify concurrency. It did simplify many things such as mutex usage, thread creation,,, but I would say it's superficial simplification.

It reduce complexity level from 9 to 8.5.

Note java concurrency is OO-concurrency. Across all languages, OO-concurrency is usually a wrapper, because core concurrency libraries, concurrency constructs.. are always written in C , at a lower level, not OO at all.

-- new or fuxi:
listComp, lambda, interactive shell ... have something in common: the Expression part is the centerpiece. It creates a return value.

-- new: set in python
set is the least used builtin container. I seldom use it, becasue dictionary is more useful.
( Similarly, list is always more useful than tuple, except when used as dict keys. )

One advantage of set over dict or list -- set-comprehension to create a hashset

-- update blogpost on java functors
Only in java is the rule very simple and clean -- FCC functions passed into a HOF must be objects.

c++, c# and py are more flexible and less simple

-- new: java Future::cancel(boolean mayInterruptIfRunning)
java Future::cancel(boolean mayInterruptIfRunning) would probably use interrupt to cancel a running thread.

If the boolean arg is false, then the thread is not interrupted and would continue running.

I think pthreads define cancellation points, similar to java's interrupt points.

-- fuxi: precedence: NOT > AND > OR
Proposition 1: Dinosaur existed
Proposition 2: speed kills

Not P1 AND Not P2 means (Not P1) and (Not P2), with or without the parentheses.

Same in any language.
-- new or .. value of real work experience
This value is lowest in concurrency design (and implementation).
Probability of observing a bad race is extremely low, so real world experience of "millions of tests" or "decades of successful operation" often gives a false sense of reassurance.

Correct design depends on peer review, which is based on the guarantee provided by the underlying infrastructure. When one of the underlying specifications changes, the correctness may be compromised without us knowing. 
-- fuxi
in GMDS, catcha mail .. a greenfield design needs to focus on the show-stopper aspects like crashing, instrumentation.

latency is not really a practical focus.

-- fuxi: These new "infrastructure" technologies are not reliable and fooolproof as traditional infrastructure technologies.
eg(big): wordpress block editor
eg: show-stopper: oversensitive touchpad 
eg: show-stopper: home wifi issues
eg: github new policies
eg: wechat issues
eg: wordpress take-down notice
eg: windows license expiry
eg: windows update kick-in at the wrong time
eg: Citrix issues
eg: M1 and starhub blackout

--fuxi
signals in windows?

real time signals are a new Posix concept... not really low-latency.

Most signals are self-sent from the same process. I think timer alarm is one. Sockets is another example. However, keyboard signals like Ctrl-C are not self-sent.

--update on placement new
I think placement new has two distinct usages
1) for data types with a ctor, it lets you construct those objects (or arrays thereof) at a given heap location without malloc

2) for primitive types like int, it informs the compiler to treat the raw memory as "int", without malloc.

2b) If you call placmeent new with an initializer then it also performs initialization at a heap location without malloc

--fuxi: sys call is not always hardware related. my linux book has a section on sys calls related to signals!
--fuxi but no point publishing:
To find a windows service name

  sc query > some.log

To fine the pid of the service name

  sc queryex WinDefend

  taskkill /F /PID 4168
  
--fuxi
Now I realize BFT is by level i.e. visit order is root to leaf. DFT is bottom-up

--new untitled
95G offer service used tibrv to send updates to GUI. Robust and fast. 
(MLP also uses messages to update GUI)

Also there was one dedicated daemon instance for each trading account. Sounds inefficient but actually acceptable scalability.

--fuxi:
Citrix became very slow when I moved to a location far from the wifi hotspot. I could see the signal strength dropping very low.

--fuxi: JVM remote debugging is not so foolproof.
GDB is not so foolproof. See my blogposts.

We need to fully embrace the reality that live debugging is often impractical.

In contrast, "echo" is the gold standard in code tracing.

--fuxi: 5GHz ^ 2.4GHz wifi
Range favors lower frequency. Bigger homes may need it.

bandwidth favors higher frequency. Video streaming.

Overcrowding in 2.4 can cause intermittent connectivity issues, but is it 5% of the time or 0.05% of the time? Not sure. Microwave did hit me.

--fuxi: python: hasSameContent(dict1, dict2): return dict1 == dict2 

--fuxi
Low latency on a multi-core machine would probably use a lot of threads in STM

Synchronization is the bread-n-butter of multi-threading. as a skill it is all about shared mutable -- improving performance in the presence of shared mutables. However, low latency avoids shared mutables.

See Doug Lea for a broader perspective

--new: untitled
is malloc a kernel service, syscall or a userland function offered in a regular library?

Can I implement my own DAM without involving the kernel? I would say some kernel sys call is needed.

Is there a cpu instruction for heap allocation? Does cpu care about heap vs stack? Yes the CPU cares about the stack.
whilst the stack data is managed via the CPU, the heap is not managed automatically. The size of the heap can also be considerably larger than the stack and the allocation of memory must be performed manually within a program. Memory also has to be freed manually, again unlike the stack.

Can check my linux book or google on brk()

--fuxi:Martin's low-latency java talk
My Questions
Q: for low latency should we avoid concurrency ?
A: yes
%%A: parallel processing in ST-Mode is good but avoid any form of synchronization 

Q: If I don’t use heap, and only use stack and static memory….?
A: heap allows sharing between threads

Q: why is JIT not available to c++?
%%A: c was not designed for JIT and ineffective 

